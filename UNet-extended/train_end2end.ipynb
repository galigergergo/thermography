{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training End-to-End Models\n",
    "\n",
    "## Checking GPU Availability\n",
    "First, let's see if we have a GPU available. It is highly recommended to utilize a GPU, although it is also possible to run this experiment on the CPU at a much slower rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import *\n",
    "from data_utils import load_e2e_data\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Current device: \" + str(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Training Data\n",
    "Here, we define the paths to the images and their corresponding masks (targets).\n",
    "The values for normalization are pre-computed from the training data in order to save time.\n",
    "In case you want to reproduce all results from the paper, you need to run this notebook several times with different amount of training data. Remember that the actual number is 10-fold higher than what the start/stop inidices suggest, since we have every sample in 10 versions with different SNRs.\n",
    "\n",
    "Because we wanted to compare the effect of using an increasing number of samples for training, we kept the validation data always the same for a fair and meaningful comparison of the results. Notice, (contrary to the training data) we don't apply any augmentation to the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(os.getcwd(), 'data', 'end2end', 'train')\n",
    "mask_dir = os.path.join(os.getcwd(), 'data', 'masks', 'train')\n",
    "\n",
    "# end2end\n",
    "normalizer = {\n",
    "        'norm_mean': [49],\n",
    "        'norm_std': [56],\n",
    "        }\n",
    "\n",
    "start = 0\n",
    "stop = 1000\n",
    "#stop = 2000\n",
    "#stop = 4000\n",
    "#stop = 8000\n",
    "train_dataset = load_e2e_data(train_dir, mask_dir, start, stop, normalizer, augment=True)\n",
    "\n",
    "# this subset is always reserved as the validation set, regardless of the number of training samples\n",
    "start = 9000\n",
    "stop = 10000\n",
    "val_dataset = load_e2e_data(train_dir, mask_dir, start, stop, normalizer)\n",
    "\n",
    "print('Number of training samples: ' + str(len(train_dataset)))\n",
    "print('Number of validation samples: ' + str(len(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Architecture\n",
    "Here, we create the actual architecture of our network, and set it up to be used in the previously determined device (cpu or cuda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will give create the compact architecture...\n",
    "model = get_cmp_thermunet().to(torch.device(device))\n",
    "\n",
    "# ...or you could use the more complex architecture\n",
    "#model = get_lrg_thermunet().to(torch.device(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Architecture\n",
    "We can have a look at the architecture and check for instance, if the number of parameters is what we expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if next(model.parameters()).is_cuda else 'cpu')\n",
    "\n",
    "summary(model, input_size=(1, 64, 256), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Finally, we can start the training procedure.\n",
    "The folder names to store the results are automatically generated and based on a timestamp.\n",
    "They are always a subfolder of the training data folder.\n",
    "\n",
    "In case you want to observe some random output samples during training, you can set the visualization_lvl to either 1 (plot validation data output samples) or 2 (plot validation and training data output samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "visualization_lvl = 1\n",
    "\n",
    "train(model, train_dataset, val_dataset, epochs, visualization_lvl)\n",
    "\n",
    "print(\"training finished...\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
